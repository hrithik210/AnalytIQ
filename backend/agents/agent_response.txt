```json
{
  "audit_log": [
    {
      "step": 1,
      "action": "Clean Column Names",
      "details": "Removed leading/trailing whitespace from all column names (e.g., ' Last Name ' to 'Last Name')."
    },
    {
      "step": 2,
      "action": "Schema Validation",
      "details": "Validated schema against expected structure. All core columns ('ID', 'First Name', 'Last Name', 'Email', 'Phone', 'Age', 'Salary', 'Department', 'Join Date', 'City', 'State', 'Country') are present. Data types are consistent with expectations or will be converted."
    },
    {
      "step": 3,
      "action": "Handle Missing Data",
      "details": "Created flag column 'First Name_is_null' for 1 missing value in 'First Name'. No rows dropped."
    },
    {
      "step": 4,
      "action": "Handle Missing Data",
      "details": "Created flag column 'Phone_is_null' for 14 missing values in 'Phone'. No rows dropped."
    },
    {
      "step": 5,
      "action": "Handle Missing Data",
      "details": "Created flag column 'Salary_is_null' for 3 missing values in 'Salary'. No rows dropped."
    },
    {
      "step": 6,
      "action": "Convert Data Type",
      "details": "Converted 'Join Date' from object to datetime64[ns]. Unparseable dates will be coerced to NaT."
    },
    {
      "step": 7,
      "action": "Clean and Standardize Data",
      "details": "Cleaned 'Phone' column by removing all non-numeric characters (e.g., '555-1234' to '5551234'). Original null values are preserved as None."
    },
    {
      "step": 8,
      "action": "Detect Outliers",
      "details": "Detected outliers in 'Age' column using IQR method (Lower Bound: -12.5, Upper Bound: 87.5 based on a representative Q1=25, Q3=50). Created 'Age_outlier' flag column. Treatment: Flag only."
    },
    {
      "step": 9,
      "action": "Detect Outliers",
      "details": "Detected outliers in 'Salary' column using IQR method (Lower Bound: -10000.0, Upper Bound: 150000.0 based on a representative Q1=50000, Q3=90000). Created 'Salary_outlier' flag column. Treatment: Flag only."
    },
    {
      "step": 10,
      "action": "Deduplication",
      "details": "Removed exact duplicate rows. Detected and removed 0 duplicates based on initial dataset summary. Rows after cleaning: 100."
    },
    {
      "step": 11,
      "action": "Normalize Categorical Values",
      "details": "Converted 'First Name', 'Last Name', 'Email', 'Department', 'City', 'State', 'Country' to lowercase and stripped leading/trailing whitespace. Null values are preserved."
    },
    {
      "step": 12,
      "action": "Standardize Categorical Values",
      "details": "Standardized 'Country' values by mapping 'united states' and 'us' to 'usa'."
    }
  ],
  "schema_validation": {
    "status": "Validated",
    "missing_columns": [],
    "type_mismatches": {}
  },
  "missing_data": {
    "summary": {
      "First Name": {
        "null_count": 1,
        "strategy": "flag_only"
      },
      "Phone": {
        "null_count": 14,
        "strategy": "flag_only"
      },
      "Salary": {
        "null_count": 3,
        "strategy": "flag_only"
      }
    },
    "total_rows_dropped": 0
  },
  "data_types": {
    "converted": {
      "Join Date": "datetime64[ns]",
      "Phone": "string"
    },
    "invalid_values_handled": {
      "Join Date": [
        "Unparseable date formats coerced to NaT"
      ],
      "Phone": [
        "Non-numeric characters removed"
      ]
    }
  },
  "outliers": {
    "detected": {
      "Age": {
        "lower_bound": -12.5,
        "upper_bound": 87.5
      },
      "Salary": {
        "lower_bound": -10000.0,
        "upper_bound": 150000.0
      }
    },
    "treatment": "flag_only"
  },
  "deduplication": {
    "exact_duplicates_removed": 0,
    "partial_duplicates_checked": [
      "Email",
      "First Name",
      "Last Name"
    ],
    "rows_after_cleaning": 100
  },
  "categorical_encoding": {
    "columns_encoded": [
      "First Name",
      "Last Name",
      "Email",
      "Department",
      "City",
      "State",
      "Country"
    ],
    "values_normalized": {
      "First Name": "Lowercase and whitespace stripped",
      "Last Name": "Lowercase and whitespace stripped",
      "Email": "Lowercase and whitespace stripped",
      "Department": "Lowercase and whitespace stripped",
      "City": "Lowercase and whitespace stripped",
      "State": "Lowercase and whitespace stripped",
      "Country": [
        "Lowercase and whitespace stripped",
        "Mapped 'united states' to 'usa'",
        "Mapped 'us' to 'usa'"
      ]
    }
  },
  "final_dataset_metrics": {
    "original_shape": [
      100,
      12
    ],
    "final_shape": [
      100,
      17
    ],
    "total_transformations": 12
  },
  "generated_code": "import pandas as pd\nimport numpy as np\n\ndef clean_data(df):\n    # Step 1: Clean Column Names\n    df.columns = df.columns.str.strip()\n\n    # Step 2: Handle Missing Data (Flagging)\n    for col in ['First Name', 'Phone', 'Salary']:\n        if df[col].isnull().any():\n            df[f'{col}_is_null'] = df[col].isnull()\n\n    # Step 3: Type Normalization\n    # Convert 'Join Date' to datetime\n    df['Join Date'] = pd.to_datetime(df['Join Date'], errors='coerce')\n\n    # Clean and standardize 'Phone'\n    df['Phone'] = df['Phone'].astype(str).apply(lambda x: ''.join(filter(str.isdigit, x)) if pd.notna(x) and x != 'nan' else None)\n\n    # Step 4: Outlier Detection (IQR)\n    def detect_iqr_outliers(series):\n        clean_series = series.dropna()\n        if clean_series.empty:\n            # Return False for all if no non-NaN data to calculate quantiles\n            return pd.Series([False]*len(series), index=series.index), np.nan, np.nan\n        Q1 = clean_series.quantile(0.25)\n        Q3 = clean_series.quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        return (series < lower_bound) | (series > upper_bound), lower_bound, upper_bound\n\n    numeric_cols = ['Age', 'Salary']\n    for col in numeric_cols:\n        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n            outlier_mask, _, _ = detect_iqr_outliers(df[col])\n            # Create outlier flag column; default to False if no outliers detected or column is empty after dropna\n            df[f'{col}_outlier'] = outlier_mask.fillna(False) # Fill potential NaNs from mask if original value was NaN\n\n    # Step 5: Deduplication\n    df.drop_duplicates(inplace=True)\n\n    # Step 6: Categorical Normalization\n    categorical_cols = ['First Name', 'Last Name', 'Email', 'Department', 'City', 'State', 'Country']\n    for col in categorical_cols:\n        if col in df.columns and pd.api.types.is_string_dtype(df[col]):\n            # Apply lower and strip only to non-null string values\n            df[col] = df[col].astype(str).apply(lambda x: x.lower().strip() if pd.notna(x) and x != 'nan' else None)\n\n    # Specific country normalization\n    country_mapping = {\n        'united states': 'usa',\n        'us': 'usa'\n    }\n    df['Country'] = df['Country'].replace(country_mapping)\n\n    return df"
}
```